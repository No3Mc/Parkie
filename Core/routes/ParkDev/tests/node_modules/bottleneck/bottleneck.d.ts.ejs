
declare module "bottleneck" {
    namespace Bottleneck {
        type Callback<T> = (err: any, result: T) => void;
        interface Strategy {
        }

        class Cluster {
            /**
             * Constructs a bottleneck Cluser using the given options.
             * @param maxConcurrent - How many requests can be running at the same time. Default: 0 (unlimited)
             * @param minTime - How long to wait after launching a request before launching another one. Default: 0ms
             * @param highWater - How long can the queue get? Default: -1 (unlimited)
             * @param strategy - Which strategy to use if the queue gets longer than the high water mark. Default: Bottleneck.strategy.LEAK
             * @param rejectOnDrop - When true if a job is dropped its callback will be called with the first argument set to an Error object. If the job was a promise it will be rejected. Default: false
             */
            constructor(maxConcurrent?: number, minTime?: number, highWater?: number, strategy?: Bottleneck.Strategy, rejectOnDrop?: boolean);

            /**
             * Returns the limiter for the specified key.
             * @param str - The limiter key.
             */
            key(str: string): Bottleneck;

            /**
             * Disables limiter garbage collection.
             */
            stopAutoCleanup(): void;

            /**
             * Enables limiter garbage collection.
             */
            startAutoCleanup(): void;

            /**
             * Deletes the limiter for the given key
             * @param str - The key
             */
            deleteKey(str: string): void;

            /**
             * Runs the give function on every limiter in the Cluster
             * @param cb - The callback function
             */
            all(cb: (limiter: Bottleneck) => void): void;

            /**
             * Returns all the keys in the Cluster
             */
            keys(): string[];
        }
    }

    class Bottleneck {
        public static readonly strategy: {
            /**
             * When submitting a new request, if the queue length reaches highWater, drop the oldest request with the lowest priority. This is useful when requests that have been waiting for too long are not important anymore. If all the queued up requests are more important than the one being added, it won't be added.
             */
            readonly LEAK: Bottleneck.Strategy;
            /**
             * Same as LEAK, except that it will only drop requests that are less important than the one being added. If all the queued up requests are as important or more than the new one, it won't be added.
             */
            readonly OVERFLOW_PRIORITY: Bottleneck.Strategy;
            /**
             * When submitting a new request, if the queue length reaches highWater, do not add the new request. This strategy totally ignores priority levels.
             */
            readonly OVERFLOW: Bottleneck.Strategy;
            /**
             * When submitting a new request, if the queue length reaches highWater, the limiter falls into "blocked mode". All queued requests are dropped and no new requests will be accepted until the limiter unblocks. It will unblock after penalty milliseconds have passed without receiving a new request. penalty is equal to 15 * minTime (or 5000 if minTime is 0) by default and can be changed by calling changePenalty(). This strategy is ideal when bruteforce attacks are to be expected. This strategy totally ignores priority levels.
             */
            readonly BLOCK: Bottleneck.Strategy;
        };

        /**
         * Constructs a new bottleneck limiter
         * @param maxConcurrent - How many requests can be running at the same time. Default: 0 (unlimited)
         * @param minTime - How long to wait after launching a request before launching another one. Default: 0ms
         * @param highWater - How long can the queue get? Default: -1 (unlimited)
         * @param strategy - Which strategy to use if the queue gets longer than the high water mark. Default: Bottleneck.strategy.LEAK
         * @param rejectOnDrop - When true if a job is dropped its callback will be called with the first argument set to an Error object. If the job was a promise it will be rejected. Default: false
         */
        constructor(maxConcurrent?: number, minTime?: number, highWater?: number, strategy?: Bottleneck.Strategy, rejectOnDrop?: boolean);

        /**
         * Returns the number of requests queued.
         * @param priority - Returns the number of requests queued with the specified priority.
         */
        nbQueued(priority?: number): number;

        /**
         * Returns the number of requests running.
         */
        nbRunning(): number;

        /**
         * If a request was added right now, would it be run immediately?
         */
        check(): boolean;

        /**
         * Is the limiter currently in "blocked mode"?
         */
        isBlocked(): boolean;

        /**
         * Cancels all queued up requests and every added request will be automatically rejected.
         * @param interrupt - If true, prevent the requests currently running from calling their callback when they're done. Default: false.
         */
        stopAll(interrupt?: boolean): Bottleneck;

        /**
         * Register an event listener.
         * @param name - The event name.
         * @param fn - The callback function.
         */
        on(name: string, fn: Function): Bottleneck;
        on(name: "empty", fn: () => void): Bottleneck;
        on(name: "idle", fn: () => void): Bottleneck;
        on(name: "dropped", fn: (dropped: any) => void): Bottleneck;
        
        /**
         * Removes all registered event listeners.
         * @param name - The optional event name to remove listeners from.
         */
        removeAllListeners(name?: string): void;

        /**
         * Changes the settings for future requests.
         * @param maxConcurrent - How many requests can be running at the same time. Default: 0 (unlimited)
         * @param minTime - How long to wait after launching a request before launching another one. Default: 0ms
         * @param highWater - How long can the queue get? Default: -1 (unlimited)
         * @param strategy - Which strategy to use if the queue gets longer than the high water mark. Default: Bottleneck.strategy.LEAK
         * @param rejectOnDrop - When true if a job is dropped its callback will be called with the first argument set to an Error object. If the job was a promise it will be rejected. Default: false
         */
        changeSettings(maxConcurrent?: number, minTime?: number, highWater?: number, strategy?: Bottleneck.Strategy, rejectOnDrop?: boolean): Bottleneck;

        /**
         * Changes the penalty value used by the BLOCK strategy.
         */
        changePenalty(penalty: number): Bottleneck;

        /**
         * Changes the reservoir count.
         */
        changeReservoir(reservoir: number): Bottleneck;

        /**
         * Adds to the reservoir count.
         */
        incrementReservoir(incrementBy: number): Bottleneck;
        
        /**
         * Chain this limiter to another.
         * @param other - The limiter that requests to this limiter must also follow.
         */
        chain(other: Bottleneck): Bottleneck;

        <%_ for (var count of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) { _%>
        submit<R<%_ for (var idx = 1; idx <= count; idx++) { _%>, A<%= idx %><%_ } _%>>(fn: (<%_ for (var idx = 1; idx <= count; idx++) { _%>arg<%= idx %>: A<%= idx %>, <% } _%>callback: Bottleneck.Callback<R>) => void<%_ for (var idx = 1; idx <= count; idx++) { _%>, arg<%= idx %>: A<%= idx %><% } _%>, callback: Bottleneck.Callback<R>): void;
        <%_ } _%>

        <%_ for (var count of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) { _%>
        schedule<R<%_ for (var idx = 1; idx <= count; idx++) { _%>, A<%= idx %><%_ } _%>>(fn: (<%= Array.apply(null, Array(count)).map((e, i) => i+1).map(i => `arg${i}: A${i}`).join(", ") %>) => PromiseLike<R><%_ for (var idx = 1; idx <= count; idx++) { _%>, arg<%= idx %>: A<%= idx %><% } _%>): Promise<R>;
        <%_ } _%>

        <%_ for (var count of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) { _%>
        submitPriority<R<%_ for (var idx = 1; idx <= count; idx++) { _%>, A<%= idx %><%_ } _%>>(priority: number, fn: (<%_ for (var idx = 1; idx <= count; idx++) { _%>arg<%= idx %>: A<%= idx %>, <% } _%>callback: Bottleneck.Callback<R>) => void<%_ for (var idx = 1; idx <= count; idx++) { _%>, arg<%= idx %>: A<%= idx %><% } _%>, callback: Bottleneck.Callback<R>): void;
        <%_ } _%>

        <%_ for (var count of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) { _%>
        schedulePriority<R<%_ for (var idx = 1; idx <= count; idx++) { _%>, A<%= idx %><%_ } _%>>(priority: number, fn: (<%= Array.apply(null, Array(count)).map((e, i) => i+1).map(i => `arg${i}: A${i}`).join(", ") %>) => PromiseLike<R><%_ for (var idx = 1; idx <= count; idx++) { _%>, arg<%= idx %>: A<%= idx %><% } _%>): Promise<R>;
        <%_ } _%>
    }

    export default Bottleneck;
}